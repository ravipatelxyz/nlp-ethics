{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "reformulated_dataset_models.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCoayWEj_LRl"
      },
      "source": [
        "# Training for the utilitarianism task on the reformulated ethics dataset\n",
        "\n",
        "Paper in which the dataset was released: https://arxiv.org/abs/2008.02275 (ICLR, Hendrycks et al., 2021)\n",
        "\n",
        "We train on various splits of the reformulated datasets to investigate their effects on performance, and assess the calibration of the reformulated dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSCeF1tt_cMa"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p56xSddOdryI"
      },
      "source": [
        "For Google Colab only:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhDPy6thdrBG"
      },
      "source": [
        "# mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# change to directory containing relevant files\n",
        "%cd '/content/drive/MyDrive/Colab Notebooks/NLP_CW2/github'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "RQVKy9kL85si"
      },
      "source": [
        "#@title Run to check we're in the correct directory\n",
        "\n",
        "try:\n",
        "    f = open(\"check_directory.txt\")\n",
        "    print('Success :)')\n",
        "except IOError:\n",
        "    print(\"Wrong directory, please try again\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4CkrDY89NCI"
      },
      "source": [
        "from pathlib import Path\n",
        "\n",
        "OUTPUT_DIR = '6_new_models/outputs'\n",
        "\n",
        "SAVE_FIGS = True # If true, Figures will be saved (overwriting previous versions) as cells are run\n",
        "FIG_DIR = f\"{OUTPUT_DIR}/figure_outputs\"\n",
        "\n",
        "RUN_DIR = f\"{OUTPUT_DIR}/run_outputs\"\n",
        "\n",
        "MODEL_DIR = f\"{OUTPUT_DIR}/model_outputs\"\n",
        "\n",
        "for dir in [OUTPUT_DIR, FIG_DIR, RUN_DIR, MODEL_DIR]:\n",
        "  Path(dir).mkdir(parents=True, exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCuF_-pGQLnn"
      },
      "source": [
        "%%capture\n",
        "\n",
        "# Installs\n",
        "\n",
        "!git clone https://github.com/hendrycks/ethics.git \n",
        "\n",
        "!pip install torch transformers pytorch-transformers # ML\n",
        "!pip install numpy matplotlib pandas scikit-learn # Data\n",
        "\n",
        "# Imports\n",
        "\n",
        "from ethics.utils import get_ids_mask, get_tokenizer, load_model\n",
        "from itertools import product\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, random_split, TensorDataset\n",
        "from transformers import AutoConfig, AutoModelForSequenceClassification, AutoTokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phUyTIPIUJTn"
      },
      "source": [
        "# Data loading functions\n",
        "\n",
        "def load_util_sentences(data_dir, name=\"train\"):\n",
        "    path = os.path.join(data_dir, \"{}.csv\".format(name))\n",
        "    df = pd.read_csv(path, header=None)\n",
        "    sentences = []\n",
        "    for i in range(df.shape[0]):\n",
        "        sentences.append(df.iloc[i, 0])\n",
        "        sentences.append(df.iloc[i, 1])\n",
        "    labels = [-1 for _ in range(len(sentences))]\n",
        "    return sentences, labels\n",
        "\n",
        "def load_process_data(args, data_dir, dataset, name=\"train\"):\n",
        "    load_fn = {\"util\": load_util_sentences}[dataset]\n",
        "    sentences, labels = load_fn(data_dir, name=name)\n",
        "    sentences = [\"[CLS] \" + s for s in sentences]\n",
        "    tokenizer = get_tokenizer(args.model)\n",
        "    ids, amasks = get_ids_mask(sentences, tokenizer, args.max_length)\n",
        "    within_bounds = [ids[i, -1] == 0 for i in range(len(ids))]\n",
        "    if np.mean(within_bounds) < 1:\n",
        "        print(\"{} fraction of examples within context window ({} tokens): {:.3f}\".format(name, args.max_length, np.mean(within_bounds)))\n",
        "    inputs, labels, masks = torch.tensor(ids), torch.tensor(labels), torch.tensor(amasks)\n",
        "\n",
        "    if \"util\" in dataset:\n",
        "        even_mask = [i for i in range(inputs.shape[0]) if i % 2 == 0]\n",
        "        odd_mask = [i for i in range(inputs.shape[0]) if i % 2 == 1]\n",
        "        even_inputs, odd_inputs = inputs[even_mask], inputs[odd_mask]\n",
        "        even_labels, odd_labels = labels[even_mask], labels[odd_mask]\n",
        "        even_masks, odd_masks = masks[even_mask], masks[odd_mask]\n",
        "        inputs = torch.stack([even_inputs, odd_inputs], axis=1)\n",
        "        labels = torch.stack([even_labels, odd_labels], axis=1)\n",
        "        masks = torch.stack([even_masks, odd_masks], axis=1)\n",
        "\n",
        "    data = TensorDataset(inputs, masks, labels)\n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGM9n_Z8UNLL"
      },
      "source": [
        "# Tensor reshaping functions\n",
        "\n",
        "def flatten(tensor):\n",
        "    tensor = torch.cat([tensor[:, 0], tensor[:, 1]])\n",
        "    return tensor\n",
        "\n",
        "def unflatten(tensor):\n",
        "    tensor = torch.stack([tensor[:tensor.shape[0] // 2], tensor[tensor.shape[0] // 2:]], axis=1)\n",
        "    return tensor\n",
        "\n",
        "# Convert hyperparams to object for compatibility\n",
        "\n",
        "class Args:\n",
        "    def __init__(self, **entries):\n",
        "        self.__dict__.update(entries)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TbV_KnaqRl0"
      },
      "source": [
        "# Redefine for each experimental setup before calling main\n",
        "task, data_dir, train_name, test_name, test_hard_name = None, None, None, None, None\n",
        "\n",
        "# Training and evaluation loop\n",
        "def main(hyperparams, verbose=True, save=False):\n",
        "\n",
        "    args = Args(**hyperparams)\n",
        "    \n",
        "    # Load model\n",
        "    model, optimizer = load_model(args)\n",
        "\n",
        "    # Load data\n",
        "    train_and_dev_data = load_process_data(args, data_dir, \"util\", train_name)\n",
        "    train_length = int(len(train_and_dev_data)*0.8)\n",
        "    dev_length = len(train_and_dev_data) - train_length\n",
        "\n",
        "    train_data, dev_data = random_split(train_and_dev_data, [train_length, dev_length])\n",
        "    test_data = load_process_data(args, data_dir, \"util\", test_name)\n",
        "    if test_hard_name:\n",
        "      test_hard_data = load_process_data(args, data_dir, \"util\", test_hard_name)\n",
        "\n",
        "    train_dataloader = DataLoader(train_data, batch_size=hyperparams['batch_size'] // 2, shuffle=True)\n",
        "    dev_dataloader = DataLoader(dev_data, batch_size=hyperparams['batch_size'] // 2, shuffle=True)\n",
        "    test_dataloader = DataLoader(test_data, batch_size=hyperparams['batch_size'] // 2, shuffle=False)\n",
        "    if test_hard_name:\n",
        "      test_hard_dataloader = DataLoader(test_hard_data, batch_size=hyperparams['batch_size'] // 2, shuffle=False)\n",
        "\n",
        "    # Store dev accuracy per epoch\n",
        "    best_dev_acc = 0\n",
        "    best_epoch = None\n",
        "    best_model = None\n",
        "    dev_accs = []\n",
        "\n",
        "    # Train and evaluate the model\n",
        "    for epoch in range(1, hyperparams['nepochs'] + 1):\n",
        "        train(model, optimizer, train_dataloader, epoch, verbose=verbose)\n",
        "        dev_acc = evaluate(model, dev_dataloader)\n",
        "        dev_accs.append(dev_acc)\n",
        "\n",
        "        print(f\"Epoch {epoch}.\\tDev accuracy {dev_acc:.4f}.\")\n",
        "\n",
        "        if dev_acc > best_dev_acc:\n",
        "          best_dev_acc = dev_acc\n",
        "          best_epoch = epoch\n",
        "          best_model = model\n",
        "\n",
        "    # Store results\n",
        "    results = hyperparams.copy()\n",
        "    \n",
        "    results['best_dev_accuracy'] = best_dev_acc\n",
        "    results['best_epoch'] = best_epoch\n",
        "\n",
        "    # Evaluate the best model on the test data\n",
        "    test_acc = evaluate(best_model, test_dataloader)\n",
        "    results['test_acc'] = test_acc\n",
        "    if test_hard_name:\n",
        "      test_hard_acc = evaluate(best_model, test_hard_dataloader)\n",
        "      results['test_hard_accuracy'] = test_hard_acc    \n",
        "\n",
        "    # Save the best model (optional)\n",
        "    if save:\n",
        "      save_path = \"{}/{}_{}_{}_{}_{}.pkl\".format(MODEL_DIR, task, results['model'], results['learning_rate'], results['batch_size'], results['nepochs'])\n",
        "      print(\"Saving model to\", save_path)\n",
        "      torch.save(best_model.module.state_dict(), save_path)\n",
        "\n",
        "    # Save the dev accuracy plot\n",
        "    if SAVE_FIGS:\n",
        "      plt.plot(range(1, results['nepochs']+1), dev_accs, color='k', linestyle='-')\n",
        "      plt.xticks(range(1, results['nepochs']+1))\n",
        "      plt.legend(['Validation'], loc='lower right')\n",
        "      plt.ylabel('Accuracy', color='k')\n",
        "      plt.xlabel('Epoch', color='k')\n",
        "      plt.title(f'''Accuracy plot (Best val acc: {best_dev_acc:.4f}) \\n \n",
        "      Model: {hyperparams['model']}, Learning rate: {hyperparams['learning_rate']},  Batch size: {hyperparams['batch_size']}''', color='k')\n",
        "\n",
        "      plt.savefig(f\"{FIG_DIR}/{task}_{hyperparams['model']}_model_{hyperparams['nepochs']}_epochs_{best_dev_acc:.4f}_devacc_ACCPLOT.png\", dpi=300, bbox_inches = \"tight\")\n",
        "      plt.show()\n",
        "\n",
        "    return results\n",
        "\n",
        "def train(model, optimizer, train_dataloader, epoch, log_interval = 10, verbose=False):\n",
        "    # Set model to training mode\n",
        "    model.train()\n",
        "    criterion = torch.nn.BCEWithLogitsLoss()\n",
        "    ntrain_steps = len(train_dataloader)\n",
        "\n",
        "    # Loop over each batch from the training set\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Copy data to GPU if needed\n",
        "        batch = tuple(t.cuda() for t in batch)\n",
        "\n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "        # reshape\n",
        "        b_input_ids = flatten(b_input_ids)\n",
        "        b_input_mask = flatten(b_input_mask)\n",
        "\n",
        "        # Zero gradient buffers\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        output = model(b_input_ids, attention_mask=b_input_mask)[0]  # dim 1\n",
        "        output = unflatten(output)\n",
        "        diffs = output[:, 0] - output[:, 1]\n",
        "        loss = criterion(diffs.squeeze(dim=1), torch.ones(diffs.shape[0]).cuda())\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Update weights\n",
        "        optimizer.step()\n",
        "\n",
        "        if step % log_interval == 0 and step > 0 and verbose:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, step, ntrain_steps, 100. * step / ntrain_steps, loss))\n",
        "\n",
        "def evaluate(model, dataloader):\n",
        "    model.eval()\n",
        "    cors = []\n",
        "\n",
        "    for step, batch in enumerate(dataloader):\n",
        "        # Copy data to GPU if needed\n",
        "        batch = tuple(t.cuda() for t in batch)\n",
        "\n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "        # reshape\n",
        "        b_input_ids = flatten(b_input_ids)\n",
        "        b_input_mask = flatten(b_input_mask)\n",
        "\n",
        "        # Forward pass\n",
        "        with torch.no_grad():\n",
        "            output = model(b_input_ids, attention_mask=b_input_mask)[0]  # dim 1\n",
        "            output = unflatten(output)\n",
        "            diffs = output[:, 0] - output[:, 1]\n",
        "            diffs = diffs.squeeze(dim=1).detach().cpu().numpy()\n",
        "        cors.append(diffs > 0)\n",
        "\n",
        "    cors = np.concatenate(cors)\n",
        "    acc = np.mean(cors)\n",
        "\n",
        "    return acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlBNpJpFAL4h"
      },
      "source": [
        "## 1: Train on reformulated dataset\n",
        "\n",
        "We train on the reformulated dataset to investigate its effect on performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZLX2oklSiRe"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAVBinW_PhME"
      },
      "source": [
        "task = \"util_reformulated_datasets\"\n",
        "\n",
        "data_dir = '4_reformulated_datasets'\n",
        "\n",
        "grid_search_results_file = f\"{RUN_DIR}/{task}_grid_search_results.csv\"\n",
        "\n",
        "train_name = \"util_train_no_test_overlap\"\n",
        "test_name = \"util_test_easy_matched\"\n",
        "test_hard_name = \"util_test_hard_matched\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLADLTIEdjQ9"
      },
      "source": [
        "### Run experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-UFiwfYS7uM"
      },
      "source": [
        "hyperparam_grid = {'model': [\"roberta-large\"], # [\"bert-base-uncased\", \"bert-large-uncased\", \"roberta-large\", \"albert-xxlarge-v2\"]\n",
        "                   'ngpus': [1],\n",
        "                   'weight_decay': [0.01],\n",
        "                   'learning_rate': [1e-5, 3e-5],\n",
        "                   'nepochs': [2, 4],\n",
        "                   'batch_size': [8, 16],\n",
        "                   'max_length': [64]}\n",
        "\n",
        "# Store results of grid search\n",
        "results_df = pd.DataFrame()\n",
        "\n",
        "for run_index, hyperparams in enumerate(ParameterGrid(hyperparam_grid)):\n",
        "\n",
        "  results = main(hyperparams, verbose=True, save=False)\n",
        "\n",
        "  results_df = results_df.append(results, ignore_index=True)\n",
        "\n",
        "# Save results of grid search to CSV\n",
        "results_df.to_csv(grid_search_results_file, mode='a', index=False, header=not os.path.exists(grid_search_results_file))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBo6VyHrF8OU"
      },
      "source": [
        "## 2: Train on unmatching scenario pairs\n",
        "\n",
        "We train on unmatching scenario pairs to investigate whether it is possible to improve the model's performance on these, since the original dataset contained only matching scenario pairs.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHHi7DIeWU54"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6UlzP_lV7BB"
      },
      "source": [
        "task = \"util_unmatching_subsplit\"\n",
        "\n",
        "data_dir = '4_reformulated_datasets/unmatching_subsplit'\n",
        "\n",
        "grid_search_results_file = f\"{RUN_DIR}/{task}_grid_search_results.csv\"\n",
        "\n",
        "train_name = \"trainsplit_unmatched_no_test_overlap\"\n",
        "test_name = \"testsplit_unmatched\"\n",
        "test_hard_name = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKpZYjSMWYzM"
      },
      "source": [
        "### Run experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXJtCGGqib_m"
      },
      "source": [
        "hyperparam_grid = {'model': [\"roberta-large\"], # [\"bert-base-uncased\", \"bert-large-uncased\", \"roberta-large\", \"albert-xxlarge-v2\"]\n",
        "                   'ngpus': [1],\n",
        "                   'weight_decay': [0.01],\n",
        "                   'learning_rate': [1e-5, 3e-5],\n",
        "                   'nepochs': [2, 4],\n",
        "                   'batch_size': [8, 16],\n",
        "                   'max_length': [64]}\n",
        "\n",
        "# Store results of grid search\n",
        "results_df = pd.DataFrame()\n",
        "\n",
        "for run_index, hyperparams in enumerate(ParameterGrid(hyperparam_grid)):\n",
        "\n",
        "  results = main(hyperparams, verbose=True, save=False)\n",
        "\n",
        "  results_df = results_df.append(results, ignore_index=True)\n",
        "\n",
        "# Save results of grid search to CSV\n",
        "results_df.to_csv(grid_search_results_file, mode='a')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeXj5h8pGFt0"
      },
      "source": [
        "## 3: Train on matching and unmatching scenario pairs\n",
        "\n",
        "We train on a combined dataset of matching and unmatching scenario pairs, to investigate whether the performance on each can be improved."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HyUQBxpiyn7d"
      },
      "source": [
        "### Combine datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afY6_J-pypX3"
      },
      "source": [
        "# Combine training datasets\n",
        "\n",
        "df1 = pd.read_csv(\"4_reformulated_datasets/util_train_no_test_overlap.csv\", header=None)\n",
        "df2 = pd.read_csv(\"4_reformulated_datasets/unmatching_subsplit/trainsplit_unmatched_no_test_overlap.csv\", header=None)\n",
        "\n",
        "df_combined = pd.concat([df1, df2], axis=0)\n",
        "\n",
        "df_combined.to_csv(f\"{OUTPUT_DIR}/util_train_combined_matched_unmatched.csv\", index=False, index_label=False, header=False)\n",
        "\n",
        "# Combine test datasets\n",
        "\n",
        "df1 = pd.read_csv(\"4_reformulated_datasets/util_test_easy_matched.csv\", header=None)\n",
        "df2 = pd.read_csv(\"4_reformulated_datasets/unmatching_subsplit/testsplit_unmatched.csv\", header=None)\n",
        "\n",
        "df_combined = pd.concat([df1, df2], ignore_index=True)\n",
        "\n",
        "df_combined.to_csv(f\"{OUTPUT_DIR}/util_test_combined_matched_unmatched.csv\", index=False, index_label=False, header=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWm7xg7ibSLH"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQuzwkkFbVhO"
      },
      "source": [
        "# Setup\n",
        "\n",
        "task = \"util_combined_split\"\n",
        "\n",
        "data_dir = OUTPUT_DIR\n",
        "\n",
        "grid_search_results_file = f\"{RUN_DIR}/{task}_grid_search_results.csv\"\n",
        "\n",
        "train_name = \"util_train_combined_matched_unmatched\"\n",
        "test_name = \"util_test_combined_matched_unmatched\"\n",
        "test_hard_name = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPPrliozzR9N"
      },
      "source": [
        "### Run experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkyWhtNT4LMD"
      },
      "source": [
        "hyperparam_grid = {'model': [\"roberta-large\"], # [\"bert-base-uncased\", \"bert-large-uncased\", \"roberta-large\", \"albert-xxlarge-v2\"]\n",
        "                   'ngpus': [1],\n",
        "                   'weight_decay': [0.01],\n",
        "                   'learning_rate': [1e-5, 3e-5],\n",
        "                   'nepochs': [2, 4],\n",
        "                   'batch_size': [8, 16],\n",
        "                   'max_length': [64]}\n",
        "\n",
        "# Store results of grid search\n",
        "results_df = pd.DataFrame()\n",
        "\n",
        "for run_index, hyperparams in enumerate(ParameterGrid(hyperparam_grid)):\n",
        "\n",
        "  results = main(hyperparams, verbose=True, save=False)\n",
        "\n",
        "  results_df = results_df.append(results, ignore_index=True)\n",
        "\n",
        "# Save results of grid search to CSV\n",
        "results_df.to_csv(grid_search_results_file, mode='a', index=False, header=not os.path.exists(grid_search_results_file))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McsEaEOdlHkJ"
      },
      "source": [
        "## 4: Assessing calibration of reformulated dataset\n",
        "\n",
        "We assess the calibration (model uncertainty against accuracy) of the reformulated dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtLBIyhytZ3m"
      },
      "source": [
        "### Generate utility values and labels for all test scenarios"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5i-Xqwh4tfTA"
      },
      "source": [
        "# Variables to specify\n",
        "\n",
        "TASK = \"util_reformulated_datasets\"\n",
        "\n",
        "MODEL_PATH = f\"{MODEL_DIR}/util_reformulated_datasets_roberta-large_1e-05_16_4.pkl\"\n",
        "\n",
        "# \"util_test_easy_matched\" for easy test dataset\n",
        "# \"util_test_hard_matched\" for hard test dataset\n",
        "# \"test_combined_unmatched\" for combined unmatched dataset\n",
        "DATASET = \"test_combined_unmatched\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oB6oXF2ZtjpU"
      },
      "source": [
        "class Args:\n",
        "  def __init__(self, task, model, path, ngpus=1, batch_size=16, max_length=64):\n",
        "    self.task = task\n",
        "    self.model = model\n",
        "    self.path = path\n",
        "    self.ngpus = ngpus\n",
        "    self.batch_size = batch_size\n",
        "    self.max_length = max_length\n",
        "\n",
        "args = Args(task=TASK, model='roberta-large', path=MODEL_PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTP8jokpCfCk"
      },
      "source": [
        "# Load all sentences and labels\n",
        "\n",
        "sentences, labels = load_util_sentences('4_reformulated_datasets', DATASET)\n",
        "\n",
        "# Load as batchable data\n",
        "\n",
        "processed_data = load_process_data(args, '4_reformulated_datasets', \"util\", DATASET)\n",
        "dataloader = DataLoader(processed_data, batch_size=args.batch_size // 2, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3vpm9P6tKZI"
      },
      "source": [
        "def load_model(args, load_path=None, cache_dir=None):\n",
        "    if cache_dir is not None:\n",
        "        config = AutoConfig.from_pretrained(args.model, num_labels=1, cache_dir=cache_dir)\n",
        "    else:\n",
        "        config = AutoConfig.from_pretrained(args.model, num_labels=1)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(args.model, config=config)\n",
        "    if load_path is not None:\n",
        "        model.load_state_dict(torch.load(load_path))\n",
        "\n",
        "    model.cuda()\n",
        "    model = torch.nn.DataParallel(model, device_ids=[i for i in range(args.ngpus)])\n",
        "\n",
        "    print('\\nPretrained model \"{}\" loaded'.format(args.model))\n",
        "\n",
        "    return model\n",
        "\n",
        "def get_model_results(args, model, dataloader, to_print=False):\n",
        "    \"\"\"\n",
        "    Run the model on the specified dataset, and return pandas dataframe of results\n",
        "    \"\"\"\n",
        "    column_names = ['sentence_good', 'sentence_bad', 'util1', 'util2', 'net_util', 'correctness']\n",
        "    results_df = pd.DataFrame(columns = column_names)\n",
        "   \n",
        "    model.eval()\n",
        "\n",
        "    for step, batch in enumerate(dataloader):\n",
        "        # Copy data to GPU if needed\n",
        "        batch = tuple(t.cuda() for t in batch)\n",
        "\n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "        # reshape\n",
        "        b_input_ids = flatten(b_input_ids)\n",
        "        b_input_mask = flatten(b_input_mask)\n",
        "\n",
        "        # Forward pass\n",
        "        with torch.no_grad():\n",
        "            output = model(b_input_ids, attention_mask=b_input_mask)[0]  # dim 1\n",
        "            output = unflatten(output).detach().cpu().numpy()\n",
        "            diffs = output[:, 0] - output[:, 1]\n",
        "            diffs = diffs.squeeze(axis=1)\n",
        "\n",
        "            util1 = output[:, 0].flatten()\n",
        "            util2 = output[:, 1].flatten()\n",
        "            net_util = (output[:, 0] - output[:, 1]).flatten()\n",
        "            correctness = (output[:, 0] > output[:, 1]).flatten().astype(int)\n",
        "\n",
        "            for i in range(len(util1)):\n",
        "\n",
        "              sentence_idx = 2*step*(args.batch_size // 2)+2*i\n",
        "\n",
        "              temp_dict = {'sentence_good': sentences[sentence_idx], 'sentence_bad': sentences[sentence_idx+1],\n",
        "                           'util1': util1[i], 'util2': util2[i], 'net_util': net_util[i], 'correctness': correctness[i]\n",
        "                           }\n",
        "\n",
        "              results_df = results_df.append(temp_dict, ignore_index=True)\n",
        "\n",
        "    return results_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_21X1GPtm4L"
      },
      "source": [
        "# Load the model\n",
        "\n",
        "model = load_model(args, load_path=MODEL_PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJAzOdpJtv7C"
      },
      "source": [
        "# Run the model on the specified dataset and save resulting dataframe to Excel\n",
        "\n",
        "df_preds = get_model_results(args, model, dataloader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rq5CP_JvN9Wj"
      },
      "source": [
        "# Save only if file doesn't already exist (to avoid overwriting)\n",
        "\n",
        "my_file = Path(f\"{RUN_DIR}/{args.task}_{args.model}_{DATASET}_preds.xlsx\")\n",
        "if my_file.is_file() == False:\n",
        "    df_preds.to_excel(f\"{RUN_DIR}/{args.task}_{args.model}_{DATASET}_preds.xlsx\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xf-HSO6AtxOA"
      },
      "source": [
        "# Display the dataframe\n",
        "\n",
        "df_preds.round(4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8i-NQRTNLUcA"
      },
      "source": [
        "### Adding new columns of interest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjX-GCIl_f6G"
      },
      "source": [
        "df_easy = pd.read_excel(f\"{RUN_DIR}/util_reformulated_datasets_roberta-large_util_test_easy_matched_preds.xlsx\", index_col=0)\n",
        "df_hard = pd.read_excel(f\"{RUN_DIR}/util_reformulated_datasets_roberta-large_util_test_hard_matched_preds.xlsx\", index_col=0)\n",
        "df_unmatched = pd.read_excel(f\"{RUN_DIR}/util_reformulated_datasets_roberta-large_test_combined_unmatched_preds.xlsx\", index_col=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NyVABZ8zB0mo"
      },
      "source": [
        "def prepare_stats(df):\n",
        "\n",
        "    # Utility differences after sigmoid\n",
        "    df[\"sigmoid_net_util\"] = 1/(1+np.exp(-df[\"net_util\"]))\n",
        "\n",
        "    # Model's certainty\n",
        "    df[\"model_certainty_sigmoid\"] = 0.5+(np.abs(df[\"sigmoid_net_util\"]-0.5))\n",
        "\n",
        "    return df\n",
        "\n",
        "df_easy = prepare_stats(df_easy)\n",
        "df_hard = prepare_stats(df_hard)\n",
        "df_unmatched = prepare_stats(df_unmatched)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osS-rvFSvjx5"
      },
      "source": [
        "df_hard[['sentence_good', 'sentence_bad', 'util1', 'util2', 'net_util', 'correctness', 'model_certainty_sigmoid']][0:10].round(4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7c2drzwLh75"
      },
      "source": [
        "### Assessing calibration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWtcdb6w_gdj"
      },
      "source": [
        "def split_in_bins(predictions, confidence):\n",
        "    num_bins = 5\n",
        "    l = np.linspace(0.5,1,num_bins+1)\n",
        "    bins = np.linspace(0.5,.9,num_bins)+.05\n",
        "\n",
        "    conf = []\n",
        "    acc = []\n",
        "    num_in_bins = []\n",
        "\n",
        "    for ind, (lower,upper) in enumerate(zip(l[:-1], l[1:])):\n",
        "        indxs = np.where((confidence<=upper) & (confidence>lower)) # B_m\n",
        "\n",
        "        this_bin_pred = predictions[indxs]\n",
        "        this_bin_conf = confidence[indxs]\n",
        "\n",
        "        # Get average confidence\n",
        "        avg_conf = np.mean(this_bin_conf)\n",
        "\n",
        "        # Get average accuracy\n",
        "        avg_acc = np.mean(this_bin_pred)\n",
        "        conf.append(avg_conf)\n",
        "        acc.append(avg_acc)\n",
        "        num_in_bins.append(len(this_bin_pred))\n",
        "    \n",
        "    return conf, acc, bins, num_in_bins\n",
        "\n",
        "def get_ECE(confidence, accuracy, num_in_bins):\n",
        "  '''\n",
        "  condifence: list of conf(B_m)\n",
        "  accuracy: list of acc(B_m)\n",
        "\n",
        "  num_in_bins: number of samples in each bin\n",
        "  '''\n",
        "  assert len(confidence) == len(accuracy)\n",
        "\n",
        "  num_in_bins = np.asarray(num_in_bins)\n",
        "  n = num_in_bins.sum() # Total number of samples\n",
        "  ECE = 0\n",
        "  for i in range(len(confidence)):\n",
        "    ECE += (num_in_bins[i]/(n)) * np.abs(accuracy[i] - confidence[i])\n",
        "\n",
        "  return ECE\n",
        "\n",
        "def plot_reliability_diagram(accuracy_easy, bins_easy, accuracy_hard, bins_hard, accuracy_unmatched, bins_unmatched):\n",
        "    accuracy = (accuracy_easy, accuracy_hard, accuracy_unmatched)\n",
        "    bins = (bins_easy, bins_hard, bins_unmatched)\n",
        "    width=0.1\n",
        "    fig, ax = plt.subplots(figsize=(13,5), nrows=1, ncols=3)\n",
        "    fig.suptitle(\"Assessing calibration of model certainty against accuracy\\n(non-Bayesian RoBERTa-large)\", fontsize=14, fontweight='bold')\n",
        "    for i in range(3):\n",
        "        ax[i].bar(bins[i], accuracy[i], width=width, color='k', edgecolor='black')\n",
        "        ax[i].plot(np.linspace(0.5,1,6),np.linspace(0.5,1,6),linestyle='--', color='red')\n",
        "        ax[i].set_ylabel(\"Accuracy\")\n",
        "        ax[i].set_xlabel(\"Model certainty\")\n",
        "        if i==0:\n",
        "            ax[i].set_title(\"Easy test dataset\")\n",
        "        if i==1:\n",
        "            ax[i].set_title(\"Hard test dataset\")\n",
        "        if i==2:\n",
        "            ax[i].set_title(\"Unmatched test dataset\")\n",
        "    fig.tight_layout(rect=[0, 0, 1, 0.90])\n",
        "    fig.legend(['Perfect certainty','Model certainty'],loc=(0.75,0.15), facecolor=\"white\")\n",
        "    if SAVE_FIGS==True:\n",
        "        fig.savefig(f\"{FIG_DIR}/hist_accuracy_v_modelcertainty\", dpi=250)\n",
        "    fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oP9bp8Tb_vQq"
      },
      "source": [
        "conf, acc, bins, num_in_bins = split_in_bins(df_easy['correctness'].to_numpy(), df_easy['model_certainty_sigmoid'].to_numpy())\n",
        "conf_hard, acc_hard, bins_hard, num_in_bins_hard = split_in_bins(df_hard['correctness'].to_numpy(), df_hard['model_certainty_sigmoid'].to_numpy())\n",
        "conf_unmatched, acc_unmatched, bins_unmatched, num_in_bins_unmatched = split_in_bins(df_unmatched['correctness'].to_numpy(), df_unmatched['model_certainty_sigmoid'].to_numpy())\n",
        "\n",
        "ece_easy = get_ECE(conf, acc, num_in_bins)\n",
        "print(f\"ECE easy test dataset: {ece_easy:.4f}\")\n",
        "\n",
        "ece_hard = get_ECE(conf_hard, acc_hard, num_in_bins_hard)\n",
        "print(f\"ECE hard test dataset: {ece_hard:.4f}\")\n",
        "\n",
        "ece_unmatched = get_ECE(conf_unmatched, acc_unmatched, num_in_bins_unmatched)\n",
        "print(f\"ECE unmatched test dataset: {ece_unmatched:.4f}\")\n",
        "\n",
        "plot_reliability_diagram(acc, bins, acc_hard, bins_hard, acc_unmatched, bins_unmatched)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}